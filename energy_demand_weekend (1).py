# -*- coding: utf-8 -*-
"""Energy_demand_weekend

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ift4R5yBTKW4A-75GT_g4DBp1DA17uHQ

Downloading libraries
"""

import pandas as pd
import numpy as np
from pandas import datetime
from matplotlib import pyplot as plt
import os

from statsmodels.tsa.arima_model import ARIMA
from matplotlib import pyplot

from sklearn.cluster import KMeans
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import MinMaxScaler
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
import statsmodels.api as sm
from statsmodels.tsa.statespace.sarimax import SARIMAX

import math

from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error

from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM

"""Reading out the csv file"""

energy = pd.read_csv('energy.csv')
energy.head(5)

#Day represents the dat data was recorded
#LCLid is the ID for the home the data was recorded from
#Energy_sum is the sum of energu used by the house in the given day

"""Count number of households participating in data as time goes by"""

#Increasing use of smart meters in london 
#Number of households which energy data was collected across different days are different
#possible interpretation that energy use is high on a given day when more data from houses is collected on a certain day
#This can be mitigated by using energy used per household
Count_houses = energy.groupby('day')[['LCLid']].nunique()
Count_houses #Some days have significantly more counts/data points that others

Count_houses.plot(figsize=(40,10))
#as shown in graph below, significant increase in people using smart meters
#More data is collected from different house holds as they grow in popularity 
#This means historical data is limited as smart meters are relatively recent

#Due to the data inconsistencies energy per household will be used as a prediction metric
energy = energy.groupby('day')[['energy_sum']].sum()
energy = energy.merge(Count_houses, on = ['day'])
energy = energy.reset_index()
energy.count()
#Data for houses is now normalized

energy

energy.day = pd.to_datetime(energy.day, format ='%Y-%m-%d').dt.date #Put time stamp on energy data
energy['avg_energy'] = energy['energy_sum']/energy['LCLid'] 
#Creating new varibale of average energy from sum of energy and how many times data is recorded for each house ID
energy.describe()

"""Read csv for weather data"""

#Displaying weather information
weather = pd.read_csv('weather_daily_darksky.csv')
weather.head(5)

weather.describe()

weather['day']=  pd.to_datetime(weather['time']) # give weather data time stamp of day
weather['day']=  pd.to_datetime(weather['day'],format='%Y%m%d').dt.date #time stamp date order

weather = weather.dropna() #drop any missing values

#merge energy usage and weather with time stamp
weather_energy =  energy.merge(weather, on=['day'])
weather_energy.head()

"""Plotting weather data with energy use"""

fig, ax1 = plt.subplots(figsize = (20,5))
ax1.plot(weather_energy.day, weather_energy.temperatureMax, color = 'tab:orange')
#Maximum tempreture recorded on that given day is displayed in orange
ax1.plot(weather_energy.day, weather_energy.temperatureMin, color = 'tab:pink')
#Minimum tempreture recorded in that given day recorded in pink
ax1.set_ylabel('Temperature')
ax1.legend()
ax2 = ax1.twinx()
ax2.plot(weather_energy.day,weather_energy.avg_energy,color = 'tab:blue')
#Average energy used by the household in blue
ax2.set_ylabel('Average Energy/Household',color = 'tab:blue')
ax2.legend(bbox_to_anchor=(0.0, 1.02, 1.0, 0.102))
plt.title('Energy Consumption and Temperature')
fig.tight_layout()
plt.show()

#CLear trent shown as reduction in temoreture typically causes an increase in energy consumption

fig, ax1 = plt.subplots(figsize = (20,5))
ax1.plot(weather_energy.day, weather_energy.cloudCover, color = 'tab:orange')
ax1.set_ylabel('Cloud Cover',color = 'tab:orange')
#Cloud cover variable shown in red
ax2 = ax1.twinx()
ax2.plot(weather_energy.day,weather_energy.avg_energy,color = 'tab:blue')
ax2.set_ylabel('Average Energy/Household',color = 'tab:blue')
plt.title('Energy Consumption and Cloud Cover')
fig.tight_layout()
plt.show()
#Correlation between cloud cover and energy consumption is evident

fig, ax1 = plt.subplots(figsize = (20,5))
ax1.plot(weather_energy.day, weather_energy.humidity, color = 'tab:orange')
ax1.set_ylabel('Humidity',color = 'tab:orange')
#Humidity shown as orange
ax2 = ax1.twinx()
ax2.plot(weather_energy.day,weather_energy.avg_energy,color = 'tab:blue')
ax2.set_ylabel('Average Energy/Household',color = 'tab:blue')
plt.title('Energy Consumption and Humidity')
fig.tight_layout()
plt.show()
#strong correlation shown between humidity levels and energy consumption
#Expected as increase use of heat during humid conditions

fig, ax1 = plt.subplots(figsize = (20,5))
ax1.plot(weather_energy.day, weather_energy.uvIndex, color = 'tab:orange')
ax1.set_ylabel('UV Index',color = 'tab:orange')
ax2 = ax1.twinx()
ax2.plot(weather_energy.day,weather_energy.avg_energy,color = 'tab:blue')
ax2.set_ylabel('Average Energy/Household',color = 'tab:blue')
plt.title('Energy Consumption and UV Index')
fig.tight_layout()
plt.show()
#Extremely high UV index indicates it is very sunny 
#In extremely sunny and hot days there is a natural increase in energy usage
#However in general when the UV index is high energy consumption is less

cor_matrix = weather_energy[['avg_energy','temperatureMax','dewPoint', 'cloudCover', 'windSpeed','pressure', 'visibility', 'humidity','uvIndex', 'moonPhase']].corr()
cor_matrix
#Correlation matrix between the features to identify which correlations are strongest

"""Read weekends and bank holiday csv /
Merge weather and calender dataset
"""

holiday = pd.read_excel('Bank_holidays_new.xlsx')
holiday['Bank holidays'] = pd.to_datetime(holiday['Bank holidays'],format='%m/%d/%Y').dt.date
holiday.head(5)
weather_energy = weather_energy.merge(holiday, left_on = 'day',right_on = 'Bank holidays',how = 'left')
weather_energy['holiday_ind'] = np.where(weather_energy['Bank holidays'].isna(),0,1)

holiday.head(10)

weather_energy

#avg_energy in last row (828) contains glaring outlier
#remove instead of replacing with average of the row
#enough data available to make removal inconsequential 
weather_energy = weather_energy.drop(828)
weather_energy.tail()

weather_energy.dtypes
#As shown all the data

#CONVERT ALL VARIABLES WITH TIME INTO PDDATETIME FORMAT
weather_energy.sunsetTime = pd.to_datetime(weather_energy.sunsetTime, format ='%Y-%m-%d').dt.date
weather_energy.temperatureMaxTime = pd.to_datetime(weather_energy.temperatureMaxTime, format ='%Y-%m-%d').dt.date
weather_energy.temperatureMinTime = pd.to_datetime(weather_energy.temperatureMinTime, format ='%Y-%m-%d').dt.date
weather_energy.apparentTemperatureMinTime = pd.to_datetime(weather_energy.apparentTemperatureMinTime, format ='%Y-%m-%d').dt.date
weather_energy.apparentTemperatureHighTime = pd.to_datetime(weather_energy.apparentTemperatureHighTime, format ='%Y-%m-%d').dt.date
weather_energy.time = pd.to_datetime(weather_energy.time, format ='%Y-%m-%d').dt.date
weather_energy.sunsetTime = pd.to_datetime(weather_energy.sunsetTime, format ='%Y-%m-%d').dt.date
weather_energy.apparentTemperatureMaxTime = pd.to_datetime(weather_energy.apparentTemperatureMaxTime, format ='%Y-%m-%d').dt.date
weather_energy.sunriseTime = pd.to_datetime(weather_energy.sunriseTime, format ='%Y-%m-%d').dt.date
weather_energy.uvIndexTime = pd.to_datetime(weather_energy.uvIndexTime, format ='%Y-%m-%d').dt.date
weather_energy.apparentTemperatureMinTime = pd.to_datetime(weather_energy.apparentTemperatureHighTime, format ='%Y-%m-%d').dt.date
weather_energy.temperatureLowTime = pd.to_datetime(weather_energy.temperatureLowTime, format ='%Y-%m-%d').dt.date
weather_energy.apparentTemperatureMaxTime = pd.to_datetime(weather_energy.apparentTemperatureMaxTime, format ='%Y-%m-%d').dt.date
weather_energy.apparentTemperatureLowTime = pd.to_datetime(weather_energy.apparentTemperatureLowTime, format ='%Y-%m-%d').dt.date
weather_energy.dtypes

weather_energy.sunsetTime.max()
weather_energy.uvIndexTime.max()

"""Multiple Linear regression"""

New_weather_energy = weather_energy.drop(['sunsetTime', 'temperatureMaxTime', 'temperatureMinTime', 'apparentTemperatureMinTime', 'apparentTemperatureHighTime','time', 'apparentTemperatureMaxTime',  'sunriseTime', 'uvIndexTime', 'apparentTemperatureMinTime', 'temperatureLowTime', 'apparentTemperatureMaxTime', 'apparentTemperatureLowTime', 'energy_sum', 'LCLid', 'summary', 'Bank holidays', 'temperatureHighTime', 'day'], axis = 1)

#categorical: icon, precipType
#removing all variables that are not of use to dataset when constructing Multiple Linear regression

from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import  OneHotEncoder
New_weather_energy = pd.get_dummies(New_weather_energy)
New_weather_energy.head()

X_multi = New_weather_energy.drop("avg_energy", axis=1)
X_multi.head()

y_multi = New_weather_energy['avg_energy']
y_multi.head()

import statsmodels.api as sn

X_multi_cons = sn.add_constant(X_multi)
X_multi_cons.head()

lm_multi = sn.OLS(y_multi, X_multi_cons).fit()
lm_multi.summary()

from sklearn.linear_model import LinearRegression

lm3 = LinearRegression()
lm3.fit(X_multi, y_multi)
print(lm3.intercept_, lm3.coef_)

"""Neural Network"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import matplotlib as mpl
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import  keras
from sklearn.preprocessing import StandardScaler

#X_train, X_test, y_train, y_test = train_test_split(X_multi, y_multi, random_state = 42)

X_train_full, X_test, y_train_full, y_test = train_test_split(X_multi, y_multi, random_state=42)
X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, random_state=42)

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_valid = scaler.fit_transform(X_valid)
X_test = scaler.fit_transform(X_test)
np.random.seed(42)
tf.random.set_seed(42)

X_train.shape

model = keras.models.Sequential([
    keras.layers.Dense(30, activation="relu", input_shape=[26]),
    keras.layers.Dense(30, activation="relu"),
    keras.layers.Dense(1)
])

model.compile(loss="mean_squared_error",
              optimizer=keras.optimizers.SGD(lr=0.01),
              metrics=['mae'])

model.summary()

model_history = model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))

mae_test = model.evaluate(X_test, y_test)

model_history.history

pd.DataFrame(model_history.history).plot(figsize=(10,10))
plt.grid(True)
plt.gca().set_ylim(0,3)
plt.show()
#Convergance achieved on this model

X_new = X_test
#Sample of the first three values of the first data set

y_pred = model.predict(X_new)
print(y_pred)
print(y_test[:10])

"""Multiple Linear Regression

"""

#Using Multiple Linear regression to predict energy demand 

#Selecting dependant and Independant variables  
X = weather_energy[['day'  ,'temperatureMax' , 'windBearing' ,	'dewPoint',
                   'cloudCover',	'windSpeed'	,'pressure'		,
                   'visibility',	'humidity',
                   'uvIndex', 	'sunsetTime',	'temperatureLow',
                   'temperatureMin'	,'temperatureHigh',	'sunriseTime'	,		
                   'moonPhase',	'holiday_ind']].values
y = weather_energy['avg_energy'].values

#BUILD MACHINE LEARNING MODDELS
#BUILD WITH NO TIME/DATE VARIABLES > WITH WEATHER AND WEATHER + HOLIDAY
# USE MULTIPLE TYPES OF REGRESSION


X_regression = weather_energy[['temperatureMax' , 'windBearing' ,	'dewPoint',
                   'cloudCover',	'windSpeed'	,'pressure'		,
                   'visibility',	'humidity',
                   'uvIndex', 		'temperatureLow',
                   'temperatureMin'	,'temperatureHigh',				
                   'moonPhase',	'holiday_ind']].values
y_regression = weather_energy['avg_energy'].values

#Splitting the dataset into testset and training set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)
y_train
#THURSDAYS WORK
#EXPLORE TIME SERIES FOR MACHINE LEARNING
#MAKE MODEL WITH TIME SERIES AND TIME AND DATE FUNCTION

#Splitting data into training set and test set
from sklearn.model_selection import train_test_split
X_train1, X_test1, y_train1, y_test1 = train_test_split(X_regression, y_regression, test_size = 0.2, random_state = 0)

#Training multiple linear regression
from sklearn.linear_model import LinearRegression
regressor1 = LinearRegression()
regressor1.fit(X_train1, y_train1)

#Predicting the test set results
y_pred1 = regressor1.predict(X_test1)
np.set_printoptions(precision=2)
print(np.concatenate((y_pred1.reshape(len(y_pred1),1), y_test1.reshape(len(y_test1),1)),1))

#Predicting the test set results
y_pred1 = regressor1.predict(X_test1)
np.set_printoptions(precision=2)
print(np.concatenate((y_pred1.reshape(len(y_pred1),1), y_test1.reshape(len(y_test1),1)),1))

X_temperatureMax = weather_energy[['temperatureMax']].values

X_windBearing = weather_energy[['windBearing']].values

X_dewPoint = weather_energy[['dewPoint']].values

X_cloudCover = weather_energy[['cloudCover']].values

X_windSpeed = weather_energy[['windSpeed']].values  

X_pressure = weather_energy[['pressure']].values

X_visibility = weather_energy[['visibility']].values  

X_humidity = weather_energy[['humidity']].values 

X_uvIndex = weather_energy[['uvIndex']].values  

X_temperatureLow = weather_energy[['temperatureLow']].values 

X_temperatureMin = weather_energy[['temperatureMin']].values  

X_temperatureHigh = weather_energy[['temperatureHigh']].values

X_moonPhase = weather_energy[['moonPhase']].values

X_holiday_ind = weather_energy[['holiday_ind']].values

y_regression = weather_energy['avg_energy'].values

# Training the Linear Regression model on the whole dataset
from sklearn.linear_model import LinearRegression
lin_reg = LinearRegression()
lin_reg.fit(X_temperatureHigh, y_regression)


# Training the Polynomial Regression model on the whole dataset
from sklearn.preprocessing import PolynomialFeatures
poly_reg = PolynomialFeatures(degree = 4)
X_poly = poly_reg.fit_transform(X_temperatureHigh)
lin_reg_2 = LinearRegression()
lin_reg_2.fit(X_poly, y_regression)

# Visualising the Linear Regression results
plt.scatter(X_temperatureHigh, y_regression, color = 'red')
plt.plot(X_temperatureHigh, lin_reg.predict(X_temperatureHigh), color = 'blue')
plt.title('(Linear Regression)')
plt.xlabel('Temperature_High')
plt.ylabel('Average Energy')
plt.show()

# Visualising the Polynomial Regression results
plt.scatter(X_temperatureHigh, y_regression, color = 'red')
plt.plot(X_temperatureHigh, lin_reg_2.predict(poly_reg.fit_transform(X_temperatureHigh)), color = 'blue')
plt.title('(Polynomial Regression)')
plt.xlabel('Temperature_high')
plt.ylabel('Average Energy')
plt.show()

# Visualising the Polynomial Regression results (for higher resolution and smoother curve)
X_grid = np.arange(min(X_temperatureHigh), max(X_temperatureHigh), 0.1)
X_grid = X_grid.reshape((len(X_grid), 1))
plt.scatter(X_temperatureHigh, y_regression, color = 'red')
plt.plot(X_grid, lin_reg_2.predict(poly_reg.fit_transform(X_grid)), color = 'blue')
plt.title('(Polynomial Regression High Resolution)')
plt.xlabel('Tempreture High')
plt.ylabel('Average Energu')
plt.show()

#PREDICTING AVERAGE ENERGY USED BASED ON TEMPERATURE HIGH
#Predicting a new result with Linear Regression

lin_reg.predict([[38]])

# Predicting a new result with Polynomial Regression
lin_reg_2.predict(poly_reg.fit_transform([[38]]))

#Prediction based on X_temperatureMax
# Training the Linear Regression model on the whole dataset
from sklearn.linear_model import LinearRegression
lin_reg = LinearRegression()
lin_reg.fit(X_temperatureMax, y_regression)


# Training the Polynomial Regression model on the whole dataset
from sklearn.preprocessing import PolynomialFeatures
poly_reg = PolynomialFeatures(degree = 4)
X_poly = poly_reg.fit_transform(X_temperatureMax)
lin_reg_2 = LinearRegression()
lin_reg_2.fit(X_poly, y_regression)

# Visualising the Linear Regression results
plt.scatter(X_temperatureMax, y_regression, color = 'red')
plt.plot(X_temperatureMax, lin_reg.predict(X_temperatureMax), color = 'blue')
plt.title('(Linear Regression)')
plt.xlabel('Temperature_High')
plt.ylabel('Average Energy')
plt.show()

# Visualising the Polynomial Regression results
plt.scatter(X_temperatureMax, y_regression, color = 'red')
plt.plot(X_temperatureMax, lin_reg_2.predict(poly_reg.fit_transform(X_temperatureMax)), color = 'blue')
plt.title('(Polynomial Regression)')
plt.xlabel('Temperature Max')
plt.ylabel('Average Energy')
plt.show()

# Visualising the Polynomial Regression results (for higher resolution and smoother curve)
X_grid = np.arange(min(X_temperatureMax), max(X_temperatureMax), 0.1)
X_grid = X_grid.reshape((len(X_grid), 1))
plt.scatter(X_temperatureMax, y_regression, color = 'red')
plt.plot(X_grid, lin_reg_2.predict(poly_reg.fit_transform(X_grid)), color = 'blue')
plt.title('(Polynomial Regression High Resolution)')
plt.xlabel('Tempreture Max')
plt.ylabel('Average Energy')
plt.show()

#PREDICTING AVERAGE ENERGY USED BASED ON TEMPERATURE HIGH
#Predicting a new result with Linear Regression

lin_reg.predict([[27]])

# Predicting a new result with Polynomial Regression
lin_reg_2.predict(poly_reg.fit_transform([[27]]))

#Prediction based on Dew point
# Training the Linear Regression model on the whole dataset
from sklearn.linear_model import LinearRegression
lin_reg = LinearRegression()
lin_reg.fit(X_dewPoint, y_regression)


# Training the Polynomial Regression model on the whole dataset
from sklearn.preprocessing import PolynomialFeatures
poly_reg = PolynomialFeatures(degree = 4)
X_poly = poly_reg.fit_transform(X_dewPoint)
lin_reg_2 = LinearRegression()
lin_reg_2.fit(X_poly, y_regression)

# Visualising the Linear Regression results
plt.scatter(X_dewPoint, y_regression, color = 'red')
plt.plot(X_dewPoint, lin_reg.predict(X_dewPoint), color = 'blue')
plt.title('(Linear Regression)')
plt.xlabel('Temperature_High')
plt.ylabel('Average Energy')
plt.show()

# Visualising the Polynomial Regression results
plt.scatter(X_dewPoint, y_regression, color = 'red')
plt.plot(X_dewPoint, lin_reg_2.predict(poly_reg.fit_transform(X_dewPoint)), color = 'blue')
plt.title('(Polynomial Regression)')
plt.xlabel('Temperature Max')
plt.ylabel('Average Energy')
plt.show()

# Visualising the Polynomial Regression results (for higher resolution and smoother curve)
X_grid = np.arange(min(X_dewPoint), max(X_dewPoint), 0.1)
X_grid = X_grid.reshape((len(X_grid), 1))
plt.scatter(X_dewPoint, y_regression, color = 'red')
plt.plot(X_grid, lin_reg_2.predict(poly_reg.fit_transform(X_grid)), color = 'blue')
plt.title('(Polynomial Regression High Resolution)')
plt.xlabel('Tempreture Max')
plt.ylabel('Average Energy')
plt.show()

#PREDICTING AVERAGE ENERGY USED BASED ON TEMPERATURE HIGH
#Predicting a new result with Linear Regression

lin_reg.predict([[27]])

# Predicting a new result with Polynomial Regression
lin_reg_2.predict(poly_reg.fit_transform([[27]]))

y_regression = y.reshape(len(y),1)
print(y)

#SVR REGRESSION

# Feature Scaling
from sklearn.preprocessing import StandardScaler
sc_X = StandardScaler()
sc_y = StandardScaler()
X = sc_X.fit_transform(X_temperatureHigh)
y = sc_y.fit_transform(y_regression)
print(X)
print(y)

# Training the SVR model on the whole dataset
from sklearn.svm import SVR
regressor = SVR(kernel = 'rbf')
regressor.fit(X, y)

# Predicting a new result
sc_y.inverse_transform(regressor.predict(sc_X.transform([[6.5]])))

# Visualising the SVR results
plt.scatter(sc_X.inverse_transform(X), sc_y.inverse_transform(y), color = 'red')
plt.plot(sc_X.inverse_transform(X), sc_y.inverse_transform(regressor.predict(X)), color = 'blue')
plt.title('Energy and Temp (SVR)')
plt.xlabel('Temp')
plt.ylabel('Average Energy')
plt.show()

# Visualising the SVR results (for higher resolution and smoother curve)
X_grid = np.arange(min(sc_X.inverse_transform(X)), max(sc_X.inverse_transform(X)), 0.1)
X_grid = X_grid.reshape((len(X_grid), 1))
plt.scatter(sc_X.inverse_transform(X), sc_y.inverse_transform(y), color = 'red')
plt.plot(X_grid, sc_y.inverse_transform(regressor.predict(sc_X.transform(X_grid))), color = 'blue')
plt.title('Energy and Temp (SVR)')
plt.xlabel('Temp')
plt.ylabel('Average Energy')
plt.show()

#DECISION TREE REGRESSION

# Training the Decision Tree Regression model on the whole dataset
from sklearn.tree import DecisionTreeRegressor
regressor = DecisionTreeRegressor(random_state = 0)
regressor.fit(X_temperatureHigh, y_regression)

# Predicting a new result
regressor.predict([[21]])

# Visualising the Decision Tree Regression results (higher resolution)
X_grid = np.arange(min(X), max(X), 0.01)
X_grid = X_grid.reshape((len(X_grid), 1))
plt.scatter(X_temperatureHigh, y_regression, color = 'red')
plt.plot(X_grid, regressor.predict(X_grid), color = 'blue')
plt.title('(Decision Tree Regression)')
plt.xlabel('Temp')
plt.ylabel('Average Energy')
plt.show()

#RANDOM FOREST 

# Training the Random Forest Regression model on the whole dataset
from sklearn.ensemble import RandomForestRegressor
regressor = RandomForestRegressor(n_estimators = 10, random_state = 0)
regressor.fit(X_temperatureHigh, y_regression)

# Predicting a new result
regressor.predict([[6.5]])

# Visualising the Random Forest Regression results (higher resolution)
X_grid = np.arange(min(X_temperatureHigh), max(X_temperatureHigh), 0.01)
X_grid = X_grid.reshape((len(X_grid), 1))
plt.scatter(X_temperatureHigh, y_regression, color = 'red')
plt.plot(X_grid, regressor.predict(X_grid), color = 'blue')
plt.title('(Random Forest Regression)')
plt.xlabel('Temp')
plt.ylabel('Average energy')
plt.show()

"""Neural Network"""

X_Nueral = weather_energy[['temperatureMax' , 'windBearing' ,	'dewPoint',
                   'cloudCover',	'windSpeed'	,'pressure'		,
                   'visibility',	'humidity',
                   'uvIndex', 		'temperatureLow',
                   'temperatureMin'	,'temperatureHigh',				
                   'moonPhase', 'avg_energy']].values
y_Nueral = weather_energy['holiday_ind'].values

#BUILD NEURAL NETWORK MODEL
# ANN MODEL IS ABLE TO DETERMINE IF THERE IS HOLIDAY OR NOT BASED ON INFORMATION

# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X_Nueral, y_Nueral, test_size = 0.2, random_state = 0)

# Feature Scaling
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

import tensorflow as tf
tf.__version__

# Part 2 - Building the ANN

# Initializing the ANN
ann = tf.keras.models.Sequential()

# Adding the input layer and the first hidden layer
ann.add(tf.keras.layers.Dense(units=6, activation='relu'))

# Adding the second hidden layer
ann.add(tf.keras.layers.Dense(units=6, activation='relu'))

# Adding the output layer
ann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))

# Part 3 - Training the ANN

# Compiling the ANN
ann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])

# Training the ANN on the Training set
ann.fit(X_train, y_train, batch_size = 32, epochs = 100)

# Part 4 - Making the predictions and evaluating the model

# Predicting the result of a single observation

"""
Using the ANN model to predict what the average energy usage will be based on these weather factors: 
TemperatureMax: 21
Wind Bearing: 237
DewPoint: 7.5
CloudCover: 0.45
WindSpeed: 5.75
pressure:  1011.25
visibility: 12.34
Humidity: 0.73
uvIndex: 2.0
temperatureLow: 11
TemperatureMin: 13
TemperatureHigh: 22
moonPhase: 0.86
avg_energy: 10.5 


Expected average energy used based on ANN:
"""

print(ann.predict(sc.transform([[22, 209, 6.8, 0.2, 2.6, 1011.25, 16.5, 1.1, 4.0, 27, 14, 32, 0.86, 14]]))) # > 0.5)

#Probability that it is a bank holiday is 19%

# Predicting the Test set results
y_pred = ann.predict(X_test)
y_pred = (y_pred > 0.5)
print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))

# Making the Confusion Matrix
from sklearn.metrics import confusion_matrix, accuracy_score
cm = confusion_matrix(y_test, y_pred)
print(cm)
accuracy_score(y_test, y_pred)

"""KNN model"""

# Splitting the dataset into the Training set and Test set
# KNN model
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X_Nueral, y_Nueral, test_size = 0.25, random_state = 0)
print(X_train)
print(y_train)
print(X_test)
print(y_test)

# Feature Scaling
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
print(X_train)
print(X_test)

# Training the K-NN model on the Training set
from sklearn.neighbors import KNeighborsClassifier
classifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)
classifier.fit(X_train, y_train)

# Predicting a new result
print(classifier.predict(sc.transform([[5, 187, 6.8, 0.45, 5.75, 1011.25, 12.34, 0.73, 4.0, 11, 5, 35, 0.86, 15]])))

# Predicting the Test set results
y_pred = classifier.predict(X_test)
print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))

# Making the Confusion Matrix
from sklearn.metrics import confusion_matrix, accuracy_score
cm = confusion_matrix(y_test, y_pred)
print(cm)
accuracy_score(y_test, y_pred)

"""SVM"""

# Training the SVM model on the Training set
from sklearn.svm import SVC
classifier = SVC(kernel = 'linear', random_state = 0)
classifier.fit(X_train, y_train)

# Predicting a new result
print(classifier.predict(sc.transform([[5, 187, 6.8, 0.45, 5.75, 1011.25, 12.34, 0.73, 4.0, 11, 5, 35, 0.86, 15]])))

# Predicting the Test set results
y_pred = classifier.predict(X_test)
print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))

# Making the Confusion Matrix
from sklearn.metrics import confusion_matrix, accuracy_score
cm = confusion_matrix(y_test, y_pred)
print(cm)
accuracy_score(y_test, y_pred)

# Training the Logistic Regression model on the Training set
from sklearn.linear_model import LogisticRegression
classifier = LogisticRegression(random_state = 0)
classifier.fit(X_train, y_train)

# Predicting a new result
print(classifier.predict(sc.transform([[5, 187, 6.8, 0.45, 5.75, 1011.25, 12.34, 0.73, 4.0, 11, 5, 35, 0.86, 15]])))

# Predicting the Test set results
y_pred = classifier.predict(X_test)
print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))

# Making the Confusion Matrix
from sklearn.metrics import confusion_matrix, accuracy_score
cm = confusion_matrix(y_test, y_pred)
print(cm)
accuracy_score(y_test, y_pred)



"""Time series"""

weather_energy.to_csv('weather_energy.csv', index=False)

ls

#Loading weather_energy data as time series

series = pd.read_csv('weather_energy.csv', header=0, parse_dates=[0], squeeze = True)
series.tail()

#remove last row which contains ang_energy outlier once more
#still remains from last download

#series = series.drop(828)

#exploring time series

series.shape

series.head(5)

series['day'].dtype

series[(series['day'] > '2011-11-26') & (series['day'] <= '2011-12-20')]

series.describe()

# Commented out IPython magic to ensure Python compatibility.
from matplotlib import pyplot as plt
# %matplotlib inline

series['avg_energy'].plot()

"""
Plotting time series chart"""

#change index to the date column
series.index = series['day']
#plot now with date as index
series['avg_energy'].plot()
#average energy at yaxis day at xaxis

"""plotting time series for particular time interval"""

#zooming in for particular time interval
#showing energy usage from winter to summer for the year 2012
winter_summer_012 = series[(series['day'] > '2012-01-01') & (series['day'] <= '2012-06-01')].copy()
winter_summer_012['avg_energy'].plot()

#showing energy usage from summer to winter for the year 2013
summer_winter_013 = series[(series['day'] > '2013-06-01') & (series['day'] <= '2013-12-01')].copy()
summer_winter_013['avg_energy'].plot()

"""Exploring trendlines"""

import seaborn as sns
series_2 = pd.read_csv('weather_energy.csv', header=0, parse_dates=[0], squeeze = True)
sns.regplot(x = series_2.index.values, y = series_2['avg_energy'])
#graph with first oder trendline

sns.regplot(x = series_2.index.values, y = series_2['avg_energy'], order = 2)
#graph with second order trendline (quadratic)

"""Exploring seasonality in data set"""

import seaborn as sns
series_2 = pd.read_csv('weather_energy.csv', header=0, parse_dates=[0], squeeze = True)
sns.regplot(x = series_2.index.values, y=series_2['avg_energy'])
sns.regplot(x = series_2.index.values, y=series_2['avg_energy'], order = 2)
#up and down trend shows seasonality

#showing changes in energy usage within a given week
two_week_change = series[(series['day'] > '2013-02-01') & (series['day'] <= '2013-02-15')].copy()
two_week_change['avg_energy'].plot()
#as shown energy usage peaks on 02 and 03 Feb which are weekends and peaks the following weekend

two_week_change_2 = series[(series['day'] > '2013-05-01') & (series['day'] <= '2013-05-15')].copy()
two_week_change_2['avg_energy'].plot()
#peak energy usage between 11 and 12 May which are also weekends

"""Creating lag plots"""

series_2['lag1'] = series_2['avg_energy'].shift(1)
#shifts all the values by one time period
series_2.head()

sns.scatterplot(x=series_2['lag1'], y=series_2['avg_energy'])
#positive correlation between the lagged values

#alternative version of doing the same thing
from pandas.plotting import lag_plot
lag_plot(series_2['avg_energy'])



"""Autocorrelation plots"""

from pandas.plotting import  autocorrelation_plot
autocorrelation_plot(series_2['avg_energy'])
#shows correlation between variables and its lag
#around every month the is a dip in correlation values
#this is due to seasonality over the year and slight ones over a week
#lows and high in autocorrelation is expected for seasonal data

"""Test Train split for this time series"""

series_2.shape[0]
#observing the amount of rows of data

train_size = int(series_2.shape[0]*0.8)
train_size
#observing the amount of training data
#80% for training size

#slicing the data for training and test set
#training and test set cannot be selected at random for time series
train = series_2[0:train_size]
test = series_2[train_size:]

series_2.head()

"""Naive forcast model"""

Naive_train_X, Naive_train_y = train['lag1'], train['avg_energy']
Naive_test_X, Naive_test_y = test['lag1'], test['avg_energy']

#walk forward validation

predictions = Naive_test_X.copy()

print(predictions)
print(Naive_test_y)
#miniture forcast model for data

from sklearn.metrics import mean_squared_error
mse = mean_squared_error(Naive_test_y, predictions)
mse
#mse is the square root of the differences between actual value and predicted value
#if advanced model gives mse value greater than this we can consider out model unable to extract information from data

from matplotlib import  pyplot
pyplot.plot(Naive_test_y)
pyplot.plot(predictions, color='red')

del series_2['lag1']
#remove lag as it is no longer needed

series_2.head()

"""ARIMA model"""

series_2['avg_energy'].plot()
#This indicates that there is a somewhat quadratic plot 
#in ARIMA model D = 2 is used, not D =1 as that is linear

from pandas.plotting import autocorrelation_plot
autocorrelation_plot(series_2['avg_energy'])
#data for about 800 valies
#correlation value with each lag decreases and increases
#confidence interval shown with the two bands
#First confidence interval crossed at around lag 80-100, hence that is the range for the q value

"""Partial autocorrelation graph"""

from statsmodels.graphics.tsaplots import  plot_pacf
plot_pacf(series_2['avg_energy'], lags = 20) #consider 20 lags for this 
#consider 7 as P since that is before it crosses the confidence interval

from statsmodels.tsa.arima_model import ARIMA

#d = 2, q = 80, p = 7
model = ARIMA(pd.DataFrame(series_2['avg_energy']), order = (7,2,3)) # order in p,d,q
model_fit = model.fit()
model_fit.summary()

residuals = model_fit.resid
residuals.plot()
residuals.describe()
#no trend or seasonality in the data
#no values missing information

#forcasting future value
output = model_fit.forecast()
output

"""Walkforward ARIMA"""

train_size = int(series_2.shape[0]*0.7)
train, test = series_2.avg_energy[0:train_size], series_2.avg_energy[train_size:]

"""SARIMA Model (seasonal ARIMA)"""

#arima can only handle trend after differentiational 
#this version allows inclusion of trend and seasonality

data = train
predict = []
for t in test:
  model = ARIMA(data, order= (7,2,3))
  model_fit = model.fit()
  y = model_fit.forecast()
  print(y[0][0])
  predict.append(y[0][0])
  data = np.append(data, t)
  data = pd.Series(data)

#for chunk below when done loading
 
from sklearn.metrics import mean_squared_error
mse = mean_squared_error(test.valuesm, predict)
mse

"""SARIMAX model"""

from statsmodels.tsa.statespace.sarimax import SARIMAX
#from statsmodels.tsa.seasonal import seasonal_decompose
series_2.index = series_2['day']

model = SARIMAX(series_2['avg_energy'], order=(7,2,3), seasonal_order=(1,1,1,12))
model_fit = model.fit()

residuals = model_fit.resid
residuals.plot()

output = model_fit.forecast()
output

future_forcast = model_fit.forecast(12)
future_forcast.plot()
#forcast for the next 12 days

yhat = model_fit.predict()

yhat.head()

pyplot.plot(series_2['avg_energy'])
pyplot.plot(yhat, color = 'red')

"""Simple Linear"""

import statsmodels.api as sn

#Feature engineering
series_2['days_from_start'] = (series_2.index - series_2.index[0]).days; series_2

#X_days = series_2['days_from_start'].values.reshape(-1, 1)
#X_Weather_variabe = series_2['cloudCover'] #could use any weather variable available
#y = series_2['avg_energy'].values

X = sn.add_constant(series_2['days_from_start']) #independant variable
X_Weather_variabe = sn.add_constant(series_2['cloudCover']) #could use any variable
lm = sm.OLS(series_2['avg_energy'], X).fit() #dependant variable
lm.summary() #summary of the model

from sklearn.linear_model import LinearRegression
y = series_2['avg_energy']
X = series_2[['days_from_start']]
X_Weather_variabe = series_2[['temperatureHigh']]

lm2 = LinearRegression()
lm2.fit(X, y)
print(lm2.intercept_, lm2.coef_)
#intercept = 12.388, coeff of X = -2.32e-05

#predict values of y
lm2.predict(X)

lm2 = LinearRegression()
lm2.fit(X_Weather_variabe, y)
print(lm2.intercept_, lm2.coef_)

lm2.predict(X_Weather_variabe)